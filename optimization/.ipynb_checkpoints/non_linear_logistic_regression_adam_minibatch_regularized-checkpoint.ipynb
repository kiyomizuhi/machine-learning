{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "* [Vanilla Gradient Descent](#vanilla)\n",
    "* [Stochastic Gradient Descent](#sgd)\n",
    "* [Minibatch Gradient Descent](#minibatch)\n",
    "* [Momentum](#momentum)\n",
    "* [Nesterov Momentum](#nag)\n",
    "* [Adagrad](#nag)\n",
    "* [RMS Prop](#rms)\n",
    "* [Adam](#adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the linear logistic regressions for 2-class classifications, we employed such bases, using $\\mathbf{x} \\leftarrow \\big[1, \\mathbf{x}^{T} \\big]^{T}$ and $\\mathbf{\\phi} \\leftarrow \\big[\\phi_{0},\\mathbf{\\phi}^{T} \\big]^{T}$,\n",
    "\n",
    "$$\\begin{align}\n",
    "p(w=1|\\mathbf{x}, \\mathbf{\\phi}) \\quad & = &\\quad Bern_{w}\\bigg[ sig\\big[\\mathbf{\\phi}^{T}\n",
    "\\mathbf{x}\\big]\\bigg]\\\\\n",
    "p(w=0|\\mathbf{x}, \\mathbf{\\phi}) \\quad & = &\\quad 1 - Bern_{w}\\bigg[ sig\\big[\\mathbf{\\phi}^{T}\n",
    "\\mathbf{x}\\big]\\bigg]\\\\\n",
    "sig\\big[\\mathbf{\\phi}^{T}\\mathbf{x}\\big] \\quad & = &\\quad  \\frac{1}{1 + exp(-\\mathbf{\\phi}^{T}\\mathbf{x})}\\\\\n",
    "Bern_{w}(\\mathbf{a}) \\quad & = &\\quad \\prod_{i}^{N} a_{i}^{w_{i}}(1 - a_{i})^{1 - w_{i}}\n",
    "\\end{align}$$\n",
    "\n",
    "We extend the linear logistic regressions to employ non-linear bases.\n",
    "\n",
    "$$\\begin{align}\n",
    "p(w=1|\\mathbf{z}, \\mathbf{\\phi}) \\quad & = &\\quad Bern_{w}\\bigg[ sig\\big[\\mathbf{\\phi}^{T}\n",
    "\\mathbf{z}\\big]\\bigg]\\\\\n",
    "p(w=0|\\mathbf{z}, \\mathbf{\\phi}) \\quad & = &\\quad 1 - Bern_{w}\\bigg[ sig\\big[\\mathbf{\\phi}^{T}\n",
    "\\mathbf{z}\\big]\\bigg]\\\\\n",
    "sig\\big[\\mathbf{\\phi}^{T}\\mathbf{z}\\big] \\quad & = &\\quad  \\frac{1}{1 + exp(-\\mathbf{\\phi}^{T}\\mathbf{z})}\n",
    "\\end{align}$$\n",
    "\n",
    "Here, for a choice of $K$ and $f$,\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{z} \\quad & = &\\quad \\big[1, f_{1}(\\mathbf{x}), \\cdots, f_{K}(\\mathbf{x})\\big]^{T}\n",
    "\\end{align}$$\n",
    "\n",
    "Among the choice of $f$'s,\n",
    "\n",
    "$$\\begin{align}\n",
    "z_{k} \\quad & = &\\quad heaviside[\\mathbf{\\alpha}_{k}^{T}\\mathbf{x}] \\quad & = &\\quad 1 \\quad if \\quad \\mathbf{\\alpha}_{k}^{T}\\mathbf{x} > 0 \\quad and \\quad 0 \\quad otherwise\\\\\n",
    "z_{k} \\quad & = &\\quad arctan[\\mathbf{\\alpha}_{k}^{T}\\mathbf{x}]\\\\\n",
    "z_{k} \\quad & = &\\quad gaussian[\\mathbf{\\alpha}_{k}^{T}\\mathbf{x}, \\lambda] \\quad & = &\\quad exp\\bigg(-\\frac{(\\mathbf{x} - \\alpha_{k})^{T}(\\mathbf{x} - \\alpha_{k})}{2\\lambda}\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "Let us concatenate all the parameters.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\theta} \\quad & = &\\quad \\big[\\mathbf{\\phi}^{T}, \\mathbf{\\alpha}_{1}^{T}, \\cdots,  \\mathbf{\\alpha}_{K}^{T} \\big]^{T}\n",
    "\\quad & = &\\quad \\big[\\phi_{0}, \\phi_{1}, \\cdots, \\phi_{K}, \\alpha_{1, 1}, \\cdots, \\alpha_{D, 1}, \\cdots, \\alpha_{1, K}, \\cdots, \\alpha_{D, K} \\big] \\quad \\Rightarrow \\quad _{(K+1 + KD) \\times 1}\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "We can choose the loglikelihood as the logistic cost function.\n",
    "\n",
    "$$\\begin{align}\n",
    "L(\\mathbf{\\theta}) \\quad & = &\\quad log \\bigg[Bern_{w}(\\mathbf{a}) \\bigg]\\\\\n",
    " \\quad & = &\\quad \\sum_{i}^{N} w_{i}log(a_{i}) + (1 - w_{i})log(1 - a_{i}) \\\\\n",
    " \\quad & = &\\quad \\sum_{i}^{N} w_{i}log\\bigg(\\frac{1}{1 + exp(-\\mathbf{\\phi}^{T}\\mathbf{z_{i}})}\\bigg) + (1 - w_{i})log \\bigg( \\frac{exp(-\\mathbf{\\phi}^{T}\\mathbf{z_{i}})}{1 + exp(-\\mathbf{\\phi}^{T}\\mathbf{z_{i}})} \\bigg) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Let us write the following explicitly for the case of $z_{i, k} = gaussian[\\mathbf{\\alpha}_{k}^{T}\\mathbf{x}_{i}, \\lambda]$\n",
    "\n",
    "$$\\begin{align}\n",
    "a_{i} \\quad & = &\\quad \\mathbf{\\phi}^{T}\\mathbf{z_{i}} \\quad & = &\\quad \\sum_{k=0}^{K} \\phi_{k}z_{i,k} \\quad & = &\\quad \\phi_{0} + \\sum_{k=1}^{K} \\phi_{k} z_{i,k}\n",
    "\\end{align}$$\n",
    "\n",
    "In order to minimize the cost function, we employ gradient descents which requires first order derivative of the cost function with respect to the parameters.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial L(\\mathbf{\\theta})}{\\partial\\theta_{j}} \\quad & = &\\quad - \\sum_{i}^{N} ( w_{i} - sig[a_{i}]) \\frac{\\partial a_{i}}{\\partial\\theta_{j}}\n",
    "\\end{align}$$\n",
    "\n",
    "When $z_{k} = gaussian[\\mathbf{\\alpha}_{k}^{T}\\mathbf{x}, \\lambda]$,\n",
    "\n",
    "$$\\begin{align}\n",
    "z_{k} \\quad & = &\\quad exp\\bigg(-\\frac{(\\mathbf{x} - \\alpha_{k})^{T}(\\mathbf{x} - \\alpha_{k})}{2\\lambda}\\bigg)\\\\\n",
    "a_{i} \\quad & = &\\quad \\phi_{0} + \\sum_{k=1}^{K} \\phi_{k} exp\\bigg(-\\frac{1}{\\lambda}\\sum_{d=1}^{D} ({x}_{i, d} - \\alpha_{d, k})^{2}\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "The gradients are\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial a_{i}}{\\partial\\theta_{\\phi}} \\quad & \\Rightarrow &\\quad \\frac{\\partial a_{i}}{\\partial\\phi_{k}} \\quad & = & \\quad \\frac{\\partial \\phi_{k}}{\\partial\\phi_{k}} z_{i, k} \\quad & = & \\quad z_{i,k} \\quad \\Rightarrow \\quad _{(K+1) \\times N} \\\\\n",
    "\\frac{\\partial a_{i}}{\\partial\\theta_{\\alpha}} \\quad & \\Rightarrow & \\quad \\frac{\\partial a_{i}}{\\partial\\mathbf{\\alpha}_{dk}} \\quad & = & \\quad \\phi_{k} \\frac{\\partial z_{i, k}}{\\partial\\alpha_{dk}} \\quad & = & \\quad \\phi_{k}\\bigg[ \\frac{z_{i, k}}{\\lambda}(x_{i,d} - \\alpha_{dk})\\bigg] \\quad \\Rightarrow \\quad _{DK \\times N}\n",
    "\\end{align}$$\n",
    "\n",
    "When $z_{k} = arctan[\\mathbf{\\alpha}_{k}^{T}\\mathbf{x}]$,\n",
    "\n",
    "$$\\begin{align}\n",
    "z_{k} \\quad & = &\\quad arctan[\\mathbf{\\alpha}_{k}^{T}\\mathbf{x}]\\\\\n",
    "a_{i} \\quad & = &\\quad \\phi_{0} + \\sum_{k=1}^{K} \\phi_{k} arctan \\bigg[\\alpha_{d, 0}\\cdot 1 + \\sum_{d=1}^{D} \\alpha_{d, k}{x}_{i, d}\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "The gradients are\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial a_{i}}{\\partial\\theta_{\\phi}} \\quad & \\Rightarrow &\\quad \\frac{\\partial a_{i}}{\\partial\\phi_{k}} \\quad & = & \\quad \\frac{\\partial \\phi_{k}}{\\partial\\phi_{k}} z_{i, k} \\quad & = & \\quad z_{i,k} \\quad \\Rightarrow \\quad _{(K+1) \\times N} \\\\\n",
    "\\frac{\\partial a_{i}}{\\partial\\theta_{\\alpha}} \\quad & \\Rightarrow & \\quad \\frac{\\partial a_{i}}{\\partial\\mathbf{\\alpha}_{dk}} \\quad & = & \\quad \\phi_{k} \\frac{\\partial z_{i, k}}{\\partial\\alpha_{dk}} \\quad & = & \\quad \\phi_{k}\\bigg[ \\frac{x_{i, d}}{1 + z_{i, k}^{2}}\\bigg] \\quad \\Rightarrow \\quad _{DK \\times N}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as spsp\n",
    "import scipy.stats as spst\n",
    "%matplotlib inline\n",
    "\n",
    "from non_linear_logistic_regression_common import *\n",
    "# usual gangs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(z, w, phi):\n",
    "    \"\"\"\n",
    "    this function computes logistic cost function\n",
    "    \n",
    "    Arguments:\n",
    "        z   :  (K + 1, N)  1 is for bias.\n",
    "        w   :  (1, N)\n",
    "        phi :  (1, K + 1)\n",
    "    \n",
    "    Output:\n",
    "        L:     scalar. cost function.\n",
    "    \"\"\"\n",
    "    sig = sigmoid(z, phi)\n",
    "    wlogsig1 = np.dot(w, np.log(sig))\n",
    "    wlogsig2 = np.dot(1 - w, np.log(1 - sig))\n",
    "    L = wlogsig1 + wlogsig2\n",
    "    return -L\n",
    "\n",
    "\n",
    "def optimization_method(method):\n",
    "    if method == \"adammini\":\n",
    "        return adam_minibatch\n",
    "    else:\n",
    "        raise KeyError(\"the method given does not exist.\")\n",
    "\n",
    "\n",
    "def optimization(z, x, w, phi, alp, lmd, sgps, nnn, beta, method):\n",
    "    \"\"\"\n",
    "    this function computes sigmoid function\n",
    "    \n",
    "    Arguments:\n",
    "        z    : generated bases including bias at the zeroth row. ((K+1), N).\n",
    "        x    : original cooridnates of the samples. (D, N)\n",
    "        phi  : phi. (1, (K+1))\n",
    "        alp  : alpha. (D, K)\n",
    "        beta : scalar. learning rate.\n",
    "        lmd  : scalar.\n",
    "        \n",
    "    Output:\n",
    "        theta: (1, (K+1+KD))\n",
    "    \"\"\"\n",
    "    D = x.shape[0]\n",
    "    D1 = D + 1\n",
    "    K = alp.shape[1]\n",
    "    K1 = K + 1\n",
    "    N = x.shape[1]\n",
    "    dlt = 1\n",
    "    step = 0\n",
    "    Ls = np.zeros(nnn)\n",
    "    alps = np.zeros((nnn, D, K))\n",
    "    alps[0] = alp\n",
    "    cost = cost_function(z, w, phi)\n",
    "    Ls[step] = cost\n",
    "    update_params = optimization_method(method)\n",
    "    v = np.zeros((1, K1 + K *D))\n",
    "    grad_sum = np.zeros((1, K1 + K *D))\n",
    "    while dlt > 0.0000001:\n",
    "        temp = cost\n",
    "        step += 1\n",
    "        if step == nnn:\n",
    "            break\n",
    "        z = radial_basis(x, alp, lmd, D, K, N)\n",
    "        bt = beta / np.sqrt(step)\n",
    "        phi, alp, grad_sum, v_new = update_params(z, x, w, phi, alp, lmd, sgps, D, K, N, beta, step, v, grad_sum)\n",
    "        cost = cost_function(z, w, phi) \n",
    "        alps[step, :, :] = alp\n",
    "        Ls[step] = cost\n",
    "        dlt = np.abs(cost - temp)/temp\n",
    "    theta = np.hstack((phi, alp.T.reshape(1, K*D)))\n",
    "    param_description(step, theta.squeeze(), lmd, sgps, beta, D, K)\n",
    "    cost = Ls[:step]\n",
    "    alpss = alps[:step, :, :]\n",
    "    return phi, alp, theta, cost, step, alpss\n",
    "\n",
    "\n",
    "def param_description(step, theta, lmd, sgps, beta, D, K):\n",
    "    K1 = K + 1\n",
    "    strng = '{:d} steps done for lambda = {:1.2f} and prior std**2 = {:1.2f}: '.format(step, lmd, sgps)\n",
    "    strng += '\\nbeta0 = {:1.2f}, beta1 = {:1.2f}, beta2 = {:1.2f}: '.format(beta[0], beta[1], beta[2])\n",
    "    strng += '\\nphi = \\n'\n",
    "    for i in range(K1):\n",
    "        strng += '{:1.2f}, '.format(theta[i])\n",
    "    strng += '\\nalp = '\n",
    "    for d in range(D):\n",
    "        strng += '\\n'\n",
    "        for i in range(K1+d, K1+K*D, D):\n",
    "            strng += '{:1.2f}, '.format(theta[i])\n",
    "    strng += '\\n'\n",
    "    print(strng)\n",
    "\n",
    "\n",
    "def adam_minibatch(z, x, w, phi, alp, lmd, sgps, D, K, N, beta, step, v, grad_sum):\n",
    "    \"\"\"\n",
    "    this function computes sigmoid function\n",
    "    \n",
    "    Arguments:\n",
    "        z    : generated bases including bias at the zeroth row. ((K+1) * N).\n",
    "        x    : original cooridnates of the samples. (D, N)\n",
    "        phi  : phi. (1, (K+1))\n",
    "        alp  : alpha. (D, K)\n",
    "        beta : scalar. learning rate.\n",
    "        lmd  : scalar.\n",
    "    \n",
    "    Intermediate:\n",
    "        dLdt   : ((K+1+KD), 1)\n",
    "\n",
    "    Output:\n",
    "        theta: (1, (K+1+KD))\n",
    "    \"\"\"\n",
    "    K1 = K + 1\n",
    "    nn = 100\n",
    "    n = np.random.choice(N, nn, replace=False)\n",
    "    x = x[:, n]\n",
    "    z = z[:, n]\n",
    "    w = w[:, n]\n",
    "    theta = np.hstack((phi, alp.T.reshape(1, K*D)))\n",
    "    dLdt  = dL_dtheta(z, x, w, phi, alp, lmd, D, K, nn)\n",
    "    v_new = beta[1] * v + (1 - beta[1]) * dLdt.T\n",
    "    v_num = v_new / (1 - beta[1] ** step)\n",
    "    grad_sum = beta[2] * grad_sum + (1 - beta[2]) * dLdt.T ** 2 \n",
    "    gs = grad_sum / (1 - beta[2] ** step)\n",
    "    theta += -beta[0] * v_num / (np.sqrt(gs) + 1e-8)  - theta / sgps\n",
    "    phi = theta[0, :K1].reshape(1, K1)\n",
    "    alp = theta[0, K1:].reshape(K, D).T\n",
    "    return phi, alp, grad_sum, v_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11cec0f98>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMm7cWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cdabe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D = 2\n",
    "K = 20\n",
    "N = 200\n",
    "nnn = 10000\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (4.0, 4.0)\n",
    "ds1, ds2, x, w = generate_data_set_dim2_nonlinear(N)\n",
    "\n",
    "alp = 2 * 2 * (np.random.rand(D, K) - 0.5)\n",
    "plt.scatter(alp[0], alp[1], s=50, c=(0.6, 0.8, 0.), marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phi = 2 * 2 * (np.random.rand(1, K + 1) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sigmoid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e92c013bf331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mradial_basis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mphio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adammini'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-0c93e3a3f299>\u001b[0m in \u001b[0;36moptimization\u001b[0;34m(z, x, w, phi, alp, lmd, sgps, nnn, beta, method)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mradial_basis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0malps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-0c93e3a3f299>\u001b[0m in \u001b[0;36madam_minibatch\u001b[0;34m(z, x, w, phi, alp, lmd, sgps, D, K, N, beta, step, v, grad_sum)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mdLdt\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdL_dtheta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0mv_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdLdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mv_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_new\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hiroyukiinoue/Documents/Gits/machine_learning/scratch/optimization/non_linear_logistic_regression_common.py\u001b[0m in \u001b[0;36mdL_dtheta\u001b[0;34m(z, x, w, phi, alp, lmd, D, K, N)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \"\"\"\n\u001b[1;32m    259\u001b[0m     \u001b[0mdadt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mda_dtheta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0msig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0mdLdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdadt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mdLdt\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sigmoid' is not defined"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 12.0)\n",
    "nnn = 1000\n",
    "betas = np.array([[0.5, 0.98, 0.98], [0.5, 0.8, 0.8], [0.5, 0.5, 0.5]])\n",
    "sgps = 1000.0\n",
    "lmd = 0.5\n",
    "ax = 0\n",
    "\n",
    "rep = betas.shape[0]\n",
    "for b in range(rep):\n",
    "    beta = betas[b, :]\n",
    "    z = radial_basis(x, alp, lmd, D, K, N)\n",
    "    phio, alpo, theta, cost, step, alps = optimization(z, x, w, phi, alp, lmd, sgps, nnn, beta, 'adammini')\n",
    "\n",
    "    ax += 1\n",
    "    plt.subplot(rep, 3, ax)\n",
    "    bd = plot_decision_boundary_dim2(phio, alpo, lmd)\n",
    "    plt.scatter(ds1[0], ds1[1], c=(1., 0., 0.), s=3)\n",
    "    plt.scatter(ds2[0], ds2[1], c=(0., 0., 1.), s=3)\n",
    "    plt.scatter(alpo[0], alpo[1], s=50, c=(1.0, 0.0, 1.0), marker='x')\n",
    "    plt.scatter(alp[0], alp[1], s=50, c=(0.0, 0.8, 0.0), marker='+')\n",
    "    plt.xlim((-3, 3))\n",
    "    plt.ylim((-3, 3))\n",
    "    plt.title('alpha = {}'.format(beta))\n",
    "\n",
    "    ax += 1\n",
    "    plt.subplot(rep, 3, ax)\n",
    "    for k in range(alps.shape[2]):\n",
    "        plt.plot(alps[:, 0, k], alps[:, 1, k], '-', lw = 0.2, color=(0.0, 0.8, 0.0))\n",
    "    initi = plt.scatter(alp[0], alp[1], s=50, c=(0.0, 0.8, 0.0), marker='+')\n",
    "    final = plt.scatter(alpo[0], alpo[1], s=80, c=(1.0, 0.0, 1.0), marker='x')\n",
    "    plt.legend([initi, final], [\"initial\", \" final\"])\n",
    "    plt.xlim((-3, 3))\n",
    "    plt.ylim((-3, 3))\n",
    "\n",
    "    ax += 1\n",
    "    plt.subplot(rep, 3, ax)\n",
    "    plt.plot(np.arange(step), cost, '.')\n",
    "    plt.xlabel('number of iterations')\n",
    "    plt.ylabel('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
