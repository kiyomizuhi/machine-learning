{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM algorithms\n",
    "\n",
    "EM algorithm is a probabilistic clustering algorithm. Given a data set $\\{\\mathbf{x}\\}$, which are suspected to be generated from parameters $\\mathbf{\\Theta}$, EM algorithm takes advantage of latent variables $\\{\\mathbf{h}\\}$, whose distribution is $q(\\{\\mathbf{h}\\})$ to maximize the likelihood $p(\\{\\mathbf{x}\\}|\\mathbf{\\Theta})$. As we do not know correct $\\mathbf{\\Theta}$, we are not able to simply compute the posterior $p(\\{\\mathbf{h}\\}| \\{\\mathbf{x}\\},\\mathbf{\\Theta})$ to obtain the optimal $q(\\{\\mathbf{h}\\})$.\n",
    "\n",
    "The EM algorithm maximize the likilihood $p(\\{\\mathbf{x}\\}|\\mathbf{\\Theta})$ by maximizing a function $B(q(\\{\\mathbf{h}\\},\\mathbf{\\Theta})$ iteratively. \n",
    "\n",
    "(1) E-step:\n",
    "\n",
    "finding the optimal distribution $q(\\{\\mathbf{h}\\})$ for a given $\\mathbf{\\Theta}$, which is equivalent to computing the posterior $p(\\{\\mathbf{h}\\}|\\{\\mathbf{x}\\},\\mathbf{\\Theta})$\n",
    "\n",
    "(2) M-step:\n",
    "\n",
    "finding the optimal $\\mathbf{\\Theta}$ for a given $q(\\{\\mathbf{h}\\})$, which is equivalent to finding $\\mathbf{\\Theta}$ that maximizes the marginalized likelihood $argmax_{\\Theta} \\int d\\{\\mathbf{h}\\} \\, q(\\{\\mathbf{h}\\})\\,log\\,p(\\{\\mathbf{x}\\} \\{\\mathbf{h}\\}|\\Theta)$ using $q(\\{\\mathbf{h}\\})$ obtained in the E-step.\n",
    "\n",
    "The iterations should continue until the marginalized likelihood saturates.\n",
    "\n",
    "Here,\n",
    "\n",
    "$B(q(\\{\\mathbf{h}\\},\\mathbf{\\Theta}) = p(\\{\\mathbf{x}\\}|\\mathbf{\\Theta}) - D_{KL}\\big(q(\\{\\mathbf{h}\\}), p(\\{\\mathbf{h}\\}, \\{\\mathbf{x}\\}|\\mathbf{\\Theta})\\big)$.\n",
    "\n",
    "$D_{KL}\\big(q(\\{\\mathbf{h}\\}), p(\\{\\mathbf{x}\\}, \\{\\mathbf{h}\\}|\\mathbf{\\Theta})\\big) > 0$ is the Kullback-Leibler divergence between $q(\\{\\mathbf{h}\\})$ and $p(\\{\\mathbf{h}\\}, \\{\\mathbf{x}\\}|\\mathbf{\\Theta})$.\n",
    "\n",
    "Once we've found the optimal $q(\\{\\mathbf{h}\\}$ and $\\mathbf{\\Theta}$, if they're correct, the second term vanishes as $q(\\{\\mathbf{h}\\} = p(\\{\\mathbf{h}\\}|\\{\\mathbf{x}\\},\\mathbf{\\Theta})$ and the first term, the likelihood $p(\\{\\mathbf{x}\\}|\\mathbf{\\Theta})$, has been maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "* [Generalities](#generalities)\n",
    "* [Theory - Gaussian Mixture (discrete mixture)](#theory_d1d)\n",
    "* [Implementation - Gaussian Mixture (discrete mixture, 1D)](#implementation_d1d)\n",
    "* [Implementation - Gaussian Mixture (discrete mixture, 2D)](#implementation_d2d)\n",
    "* [Theory - Gaussian Mixture (continuous mixture in covariance)](#theory_cc2d)\n",
    "* [Implementation - Gaussian Mixture (continuous mixture in covariance)](#implementation_cc2d)\n",
    "* [Theory - Gaussian Mixture (continuous mixture in mean) AKA Factor Analysis ](#theory_cm2d\")\n",
    "* [Implementation - Gaussian Mixture (continuous mixture in mean) in 2D](#implementation_cm2d)\n",
    "* [Theory : Discrete Student's-T Mixture ](#theory_stmd\")\n",
    "* [I Implementation : Discrete Student's-T Mixture in 2D ](#implementation_stmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalities <a class=\"anchor\" id=\"generalities\"></a>\n",
    "## Gaussian mixtures using discrete and continuous latent variables\n",
    "\n",
    "Here we describe the theory and its application for Gaussian mixtures. Suppose we obtain a data set $\\{x_{i}\\}_{i=1}^{N}$. We suspect that they were generated by some hyperparameters ${\\Theta}$, which can be expressed as a marginalization with respect to parameters ${h_{i}}$.\n",
    "\n",
    "$$\\begin{align}\n",
    "    p(x_{i}|\\Theta) \\quad & = & \\int dh_{i} \\, p(x_{i}|h_{i}) p(h_{i}|\\Theta)  \\quad \\quad (h_{i}\\, : \\, continuous)\\\\\n",
    "    p(x_{i}|\\Theta) \\quad & = & \\sum_{h_{i}} p(x_{i}|h_{i}) p(h_{i}|\\Theta)  \\quad \\quad (h_{i} \\, : \\, discrete)\n",
    "\\end{align}$$\n",
    "\n",
    "In this note, we consider three specific cases:<br>\n",
    "1) $h_{i}$ : discrete $\\Rightarrow$ discrete gaussian mixture (different means and covariances)<br>\n",
    "2) $h_{i}$ : continuous $\\Rightarrow$ continuous gaussian mixture (same mean and different covariances)<br>\n",
    "3) $h_{i}$ : continuous $\\Rightarrow$ continuous gaussian mixture (same covariance and different means)<br>\n",
    "\n",
    "In the following, we deal with the continuous case and notice the joint probability $p(x_{i}|h_{i}) p(h_{i}|\\Theta) = p(x_{i}, h_{i}|\\Theta)$.\n",
    "\n",
    "$$\\begin{align}\n",
    "L \\quad & = & \\quad \\prod_{i=1}^{N} p(x_{i}|\\Theta) \\qquad = \\qquad \\prod_{i=1}^{N} \\Big[ \\int dh_{i} \\, p(x_{i}, h_{i}|\\Theta)\\Big]\n",
    "\\end{align}$$\n",
    "\n",
    "It is more convenient to consider the log likelihood $log(L)$ to perform the maximum likelihood estimation for $\\hat\\Theta$.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat{\\Theta}\\quad & = &\\quad argmax_{\\Theta}\\quad \\sum_{i=1}^{N} log  \\, p(x_{i}|\\Theta)\\\\\n",
    "& = & argmax_{\\Theta}\\quad \\sum_{i=1}^{N} log\\Big[ \\int dh_{i} \\, p(x_{i}, h_{i}|\\Theta)\\Big]\n",
    "\\end{align}$$\n",
    "\n",
    "For a convecx function as log, Jensen's theorem holds. By multiplying and dividing the integrand by $q(h_{i})$, we can apply the Jensen's theorem. The following is true for any  $\\Theta$.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sum_{i=1}^{N} log \\Big[ \\int dh_{i} \\, p(x_{i}, h_{i}|\\Theta)\\Big] \\quad\n",
    "& \\ge &  \\sum_{i=1}^{N} \\int dh_{i} \\, q(h_{i})\\,log\\,\\bigg[  \\frac{p(x_{i}, h_{i}|\\Theta)}{q(h_{i})}\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "Define the right hand side as lower bound function $B(\\Theta, \\{h\\})$. What we would like to obtain is \n",
    "\n",
    "$$\\begin{align}\n",
    "\\{\\hat{h}_{i}\\} \\quad & = &\\quad argmax_{\\{h\\}}\\quad B(\\Theta, \\{h_{i}\\})\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat{\\Theta} \\quad & = &\\quad argmax_{\\Theta}\\quad B(\\Theta, \\{h_{i}\\})\n",
    "\\end{align}$$\n",
    "\n",
    "The strategy of the EM algorithm is to iteratively solve these equations by two steps of optimizations (maximizations) for $\\{h_{i}\\}$ and $\\Theta$ respectively. At $n$th iteration, suppose we've obtained $\\Theta_{n}$ and $\\{h\\}_{n}$ from $n-1$ th iteration.\n",
    "\n",
    "E-step:\n",
    "$$\\begin{align}\n",
    "\\{h_{i}\\}_{n + 1} \\quad & = &\\quad argmax_{\\{h\\}_{n}}\\quad B(\\Theta_{n}, \\{h_{i}\\}_{n})\n",
    "\\end{align}$$\n",
    "\n",
    "M-step:\n",
    "$$\\begin{align}\n",
    "\\Theta_{n + 1}\n",
    " \\quad & = &\\quad argmax_{\\Theta_{n}}\\quad B(\\Theta_{n}, \\{h_{i}\\}_{n+1})\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "As can be seen, \n",
    "The lower bound function for the likelihood reads\n",
    "$$\\begin{align}\n",
    "B(\\Theta, \\{h_{i}\\}) \\quad & = &\\quad \\sum_{i=1}^{N} \\int dh_{i} \\, q_{i}(h_{i})\\,log\\,\\bigg[  \\frac{p(h_{i}|x_{i}, \\Theta)p(x_{i}|\\Theta)}{q_{i}(h_{i})}\\bigg]\\\\\n",
    "& = & \\quad \\sum_{i=1}^{N} \\int dh_{i} \\, q_{i}(h_{i})\\,log\\,p(x_{i}|\\Theta) \\quad +\\quad  \\sum_{i=1}^{N} \\int dh_{i} \\, q_{i}(h_{i})\\,log\\,\\bigg[  \\frac{p(h_{i}|x_{i}, \\Theta)}{q_{i}(h_{i})}\\bigg]\\\\\n",
    "& = & \\quad \\sum_{i=1}^{N} log\\,p(x_{i}|\\Theta) \\quad +\\quad \\sum_{i=1}^{N} \\int dh_{i} \\, q_{i}(h_{i})\\,log\\,\\bigg[  \\frac{p(h_{i}|x_{i}, \\Theta)}{q_{i}(h_{i})}\\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "The first term is $h_{i}$-independent part and the second term is $h_{i}$-dependent, which is called Kullback-Leibler divergence between $p(h_{i}|x_{i}, \\Theta)$ and $q(h_{i})$. One can show that the second term is negative or equal to zero. Namely, the maximum value of the second term is zero, which is satisfied only if $q_{i}(h_{i}) = p(h_{i}|x_{i}, \\Theta_{n})$ as seee in $log \\frac{p(h_{i, n+1}|x_{i}, \\Theta)}{p(h_{i, n+1}|x_{i}, \\Theta)} = log(1) = 0$. Therefore, after the $n$th E-step,\n",
    "\n",
    "$$\\begin{align}\n",
    "B(\\Theta_{n}, \\{h_{i}\\}_{n+1}) \\quad & = &\\quad \\sum_{i=1}^{N} log\\,p(x_{i}|\\Theta_{n})\n",
    "\\end{align}$$\n",
    "\n",
    "is achieved. Notice that $\\Theta_{n}$ is not necessarily yet fully optimized to $\\hat{\\Theta}$. However, we have an issue here. We started the EM algorithm as it is difficult to directly optimize $log\\,p(x_{i}|\\Theta)$ with respect to $\\Theta$. Also, once we start tuning $\\Theta_{n}$, $q_{i}(h_{i}) = p(h_{i}|x_{i}, \\Theta_{n})$ would not be satisfied any more. Therefore, for the M-step, we have to maximize $B(\\Theta_{n}, \\{h_{i}\\}_{n+1})$ directly. In conclusion, the EM algorithm interates the following two steps until the changes in the parameters become negligible.\n",
    "<br>\n",
    "\n",
    "E-step: (optimize the contribution of each gaussian for each sample)\n",
    "<br>\n",
    "<br>\n",
    "$$\\begin{align}\n",
    "\\{h_{i}\\}_{n + 1} \\quad & = &\\quad p(h_{i, n}|x_{i}, \\Theta_{n})\n",
    "\\end{align}$$\n",
    "<br>\n",
    "M-step: (optimize the parameters ($\\lambda$, $\\mu$ and $\\Sigma$) of the Gaussians)\n",
    "<br>\n",
    "<br>\n",
    "$$\\begin{align}\n",
    "\\Theta_{n + 1}  \\quad & = &\\quad argmax_{\\Theta_{n}}\\quad \\sum_{i=1}^{N} \\int dh_{i, n+1} \\, q(h_{i, n+1})\\,log\\,p(x_{i}, h_{i, n + 1}|\\Theta_{n})\n",
    "\\end{align}$$\n",
    "\n",
    "The E-step calculates the posterior of $h_{i}$ given $x_{i}$ and $\\Theta_{n}$. For the M-step, we have dropped $q(h_{i, n+1})logq(h_{i, n+1})$ as it does not depend on $\\Theta$.\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
