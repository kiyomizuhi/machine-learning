# machine-learning

Here I follow the following book and implement the algorithms therein (mainly in the chapter 2) from scratch as a means to grasp the concepts of the probabilistic models "under the hood" of higher-level libraries and to familiarize myself with translating the math to Numpy, Scipy and visualizations. I found this book very concise and logically transparent, which should serve as a good place to exercise above.

Computer Vision: Models, Learning, and Inference by Dr Simon J. D. Prince https://www.amazon.com/dp/1107011795/ref=cm_sw_r_tw_dp_U_x_.jwjAbJCPWTQR
http://www.computervisionmodels.com/
The images below are taken from this book.


## EM algorithms (discrete & continuous latent variables)
  - Generalities
  - Gaussian Mixture (discrete mixture) 
  - Gaussian Mixture (continuous mixture in covariance)
  - Gaussian Mixture (continuous mixture in mean) AKA Factor Analysis
  - Discrete Student's-T Mixture

<img width="804" alt="screen shot 2017-12-10 at 2 25 29 am" src="https://user-images.githubusercontent.com/19827262/33800827-9749f2e6-dd51-11e7-9953-e748536d9676.png">

## Regression models
  - Maximum Likelihood Linear Regression
  - Bayesian linear regression
  - Non-linear regression
  - Kernel trick and Gaussian process regression
  - Sparse linear regression
  - Dual linear regression
  - Relevance vector regression

<img width="747" alt="screen shot 2017-12-10 at 2 29 01 am" src="https://user-images.githubusercontent.com/19827262/33800840-ebc26236-dd51-11e7-8898-a626bff9dd8c.png">

## Classification models
  - Maximum Likelihood logistic regression
  - Bayesian logistic regression
  - Non-linear logistic regression
  - Dual logistic regression
  - Relevance vector classification
  - Incremental fitting and boosting
  - Classification trees
  - Multi-class logistic regression
  - Random trees, forests, and ferns
  - Multi-class logistic regression
  
  <img width="924" alt="screen shot 2017-12-14 at 2 19 11 pm" src="https://user-images.githubusercontent.com/19827262/33992106-fff0c5c8-e0d9-11e7-87f0-2f56b13a0f8c.png">


## Gradient descent optimizations
This section is based on CS231 Convolutional Neural Networks for Visual Recognition.<br>
http://cs231n.github.io/neural-networks-3/

- Vanilla Gradient Descent
- Stochastic Gradient Descent
- Minibatch Gradient Descent
- Momentum
- Nesterov Momentum
- Adagrad
- RMS Prop
- Adam

The image below is taken from the link above.
<img width="721" alt="screen shot 2017-12-19" src="https://user-images.githubusercontent.com/19827262/34137947-f0a09874-e475-11e7-9a3d-f12c54b004c0.png">

